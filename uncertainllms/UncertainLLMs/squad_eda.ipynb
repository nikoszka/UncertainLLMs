{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "from lib2to3.pgen2.tokenize import tokenize\n",
    "\n",
    "import accelerate\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "import wandb\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6fb757-647f-4ecd-9dcd-a8a14a8c8642",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model='Llama-2-7b-hf'\n",
    "data_dir = 'c/Users/nikos/PycharmProjects/UncertainLLMs/data'\n",
    "hf_datasets_cache = 'mnt/c/Users/nikos/.cache/huggingface/datasets'\n",
    "output_dir = '/c/Users/nikos/PycharmProjects/UncertainLLMs/data'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"meta-llama/{model}\",\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             cache_dir=hf_datasets_cache).cuda()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a0e0c71-f7e7-44b9-9f11-aae6ff3a608f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 776/776 [00:00<?, ?B/s]\n",
      "C:\\Users\\nikos\\.conda\\envs\\UncertainLLMs\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nikos\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "tokenizer.model: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500k/500k [00:00<00:00, 6.92MB/s]\n",
      "tokenizer.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.84M/1.84M [00:00<00:00, 3.48MB/s]\n",
      "special_tokens_map.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 414/414 [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7662377-ee4b-44c1-9db6-b6e5ecdc35c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded squad2 dataset\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_from_disk('C:\\\\Users\\\\nikos\\\\PycharmProjects\\\\UncertainLLMs\\\\data\\\\squad_v2\\\\squad2_dataset')\n",
    "print('Loaded squad2 dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3721765f-567c-4783-b36d-cc945a9cf196",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.train_test_split(test_size=(1 - 0.1), seed=123)['train']\n",
    "questions = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed7e4ff8-b11c-47cd-a019-146d7e6c4936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5.67k/5.67k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(questions, batch_size=1)\n",
    "period_token_id = tokenizer('. ')['input_ids'][1]\n",
    "eos_tokens = ['Question:', ' Question:', '\\n', 'Answer:', ' Answer:', 'Q:']\n",
    "question_framing_ids = [[tokenizer(eos_token)['input_ids'][1]] for eos_token in eos_tokens]\n",
    "squad_metric = evaluate.load(\"squad\")\n",
    "rouge = evaluate.load('rouge')\n",
    "exact_match_metric = evaluate.load(\"exact_match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d43293e9-7c90-4990-b74d-ad36e349336b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1187/1187 [00:01<00:00, 1023.99it/s]\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'squad2':\n",
    "    print(dataset)\n",
    "else: print('false')\n",
    "\n",
    "with torch.no_grad():\n",
    "    max_length_of_generated_sequence = 256\n",
    "    sequences = []\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        input_ids = torch.cat(batch['id']).to(device).reshape(\n",
    "            1, -1) if dataset == 'squad2' else batch['input_ids'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "619e9531-57ba-4a57-b2d9-bd53082c7b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\.conda\\envs\\UncertainLLMs\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikos\\.conda\\envs\\UncertainLLMs\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "most_likely_generation = model.generate(input_ids,\n",
    "                                        num_beams=5,\n",
    "                                        num_return_sequences=2,\n",
    "                                        do_sample=False,\n",
    "                                        max_length=input_ids.shape[1] +\n",
    "                                        max_length_of_generated_sequence,\n",
    "                                        eos_token_id=period_token_id,\n",
    "                                        bad_words_ids=question_framing_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f4260-0fa9-4032-add8-8c3673f170a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f016bfc-f346-4b94-95ad-89ad5fd90513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
